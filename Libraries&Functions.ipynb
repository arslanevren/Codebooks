{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ea1709",
   "metadata": {},
   "source": [
    "# Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Installments\n",
    "!pip install pyforest\n",
    "!pip install squarify\n",
    "!pip install pyclustertend\n",
    "!pip install catboost\n",
    "!pip install optuna\n",
    "!pip install pandas_profiling\n",
    "!pip install termcolor\n",
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e807695",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycaret[full]\n",
    "!pip install pycaret-nightly\n",
    "# # https://pycaret.readthedocs.io/en/latest/index.html#\n",
    "# # install the nightly build\n",
    "# pip install pycaret-nightly\n",
    "# # install the full version of the nightly build\n",
    "# pip install pycaret-nightly[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Installments for Plotly\n",
    "!pip install --upgrade plotly\n",
    "!pip install jupyter-dash\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1645a91",
   "metadata": {},
   "source": [
    "# Libraries - ML Regression|Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316868fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraies\n",
    "\n",
    "## import all main libraries automatically with pyforest\n",
    "# !pip install pyforest\n",
    "import pyforest\n",
    "\n",
    "## main libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# !pip install squarify\n",
    "import squarify as sq\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pyclustertend import hopkins\n",
    "\n",
    "## pre-processing\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "## feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n",
    "\n",
    "## scaling\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "## regression/prediction\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "## ann\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "## classification\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree \n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "## metrics\n",
    "from sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "## model selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n",
    "\n",
    "## MLearning\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "import optuna\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "## clevers\n",
    "# !pip install -U pandas-profiling --user\n",
    "import pandas_profiling\n",
    "from pandas_profiling.report.presentation.flavours.html.templates import create_html_assets\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import missingno as msno \n",
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# !pip install termcolor\n",
    "import colorama\n",
    "from colorama import Fore, Style  # makes strings colored\n",
    "from termcolor import colored\n",
    "from termcolor import cprint\n",
    "# grey red green yellow blue magenta cyan white (on_grey ..)\n",
    "# bold dark underline blink reverse concealed\n",
    "# cprint(\"Have a first look to:\",\"blue\",\"on_grey\", attrs=['bold'])\n",
    "\n",
    "## plotly and cufflinks\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly.offline import iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "## Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "## Figure&Display options\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a4849",
   "metadata": {},
   "source": [
    "# Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Useful User-Defined-Functions\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def missing_values(df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n",
    "    return missing_values[missing_values['Missing_Number']>0]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def first_looking(df):\n",
    "    print(colored(\"Shape:\", 'yellow', attrs=['bold']), df.shape,'\\n', \n",
    "          colored('*'*100, 'red', attrs=['bold']),\n",
    "          colored(\"\\nInfo:\\n\",'yellow', attrs=['bold']), sep='')\n",
    "    print(df.info(), '\\n', \n",
    "          colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Number of Uniques:\\n\", 'yellow', attrs=['bold']), df.nunique(),'\\n',\n",
    "          colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Missing Values:\\n\", 'yellow', attrs=['bold']), missing_values(df),'\\n', \n",
    "          colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"All Columns:\", 'yellow', attrs=['bold']), *list(df.columns), sep='\\n- ') \n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n",
    "    \n",
    "    print(colored(\"Columns after rename:\", 'yellow', attrs=['bold']), *list(df.columns), sep='\\n- ')\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    \n",
    "###############################################################################\n",
    "## To view summary information about the columns\n",
    "\n",
    "def summary(column):\n",
    "    print(colored(\"Column: \",'yellow', attrs=['bold']), column)\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Missing values: \", 'yellow', attrs=['bold']), df[column].isnull().sum())\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Missing values(%): \", 'yellow', attrs=['bold']), round(df[column].isnull().sum()/df.shape[0]*100, 2))\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Unique values: \", 'yellow', attrs=['bold']), df[column].nunique())\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Value counts: \\n\", 'yellow', attrs=['bold']), df[column].value_counts(dropna = False), sep='')\n",
    "    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    \n",
    "###############################################################################\n",
    "                    \n",
    "def multicolinearity_control(df):                    \n",
    "    df_temp = df.corr()\n",
    "    count = 'Done'\n",
    "    feature =[]\n",
    "    collinear= []\n",
    "    for col in df_temp.columns:\n",
    "        for i in df_temp.index:\n",
    "            if abs(df_temp[col][i] > .8 and df_temp[col][i] < 1):\n",
    "                    feature.append(col)\n",
    "                    collinear.append(i)\n",
    "                    cprint(f\"multicolinearity alert in between {col} - {i}\", \"red\", attrs=[\"bold\"])\n",
    "    else:\n",
    "        cprint(f\"There is NO multicollinearity problem.\", \"blue\", attrs=[\"bold\"])                     \n",
    "                    \n",
    "###############################################################################\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(colored(\"Duplicate check...\", 'yellow', attrs=['bold']), sep='')\n",
    "    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n",
    "    if duplicate_values > 0:\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n",
    "              colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "    else:\n",
    "        print(colored(\"There are no duplicates\"),'\\n',\n",
    "              colored('*'*100, 'red', attrs=['bold']), sep='')     \n",
    "\n",
    "###############################################################################\n",
    "        \n",
    "def drop_columns(df, drop_columns):\n",
    "    if drop_columns !=[]:\n",
    "        df.drop(drop_columns, axis=1, inplace=True)\n",
    "        print(drop_columns, 'were dropped')\n",
    "    else:\n",
    "        print(colored('Missing value control...', 'yellow', attrs=['bold']),'\\n',\n",
    "              colored('If there is a missing value above the limit you have given, the relevant columns are dropped and an information is given.'), sep='')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def drop_null(df, limit):\n",
    "    for i in df.isnull().sum().index:\n",
    "        if (df.isnull().sum()[i]/df.shape[0]*100)>limit:\n",
    "            print(df.isnull().sum()[i], 'percent of', i ,'were null and dropped')\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "    print(colored('Last shape after missing value control:', 'yellow', attrs=['bold']), df.shape, '\\n', \n",
    "          colored('*'*100, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def shape_control():\n",
    "    print('df.shape:', df.shape)\n",
    "    print('X.shape:', X.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    print('X_train.shape:', X_train.shape)\n",
    "    print('y_train.shape:', y_train.shape)\n",
    "    print('X_test.shape:', X_test.shape)\n",
    "    print('y_test.shape:', y_test.shape)\n",
    "\n",
    "###############################################################################  \n",
    "\n",
    "## show values in bar graphic\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.2f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "        \n",
    "###############################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad5a5",
   "metadata": {},
   "source": [
    "## Fill Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0b5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def fill_most(df, group_col, col_name):\n",
    "    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n",
    "    for group in list(df[group_col].unique()):\n",
    "        cond = df[group_col]==group\n",
    "        mode = list(df[cond][col_name].mode())\n",
    "        if mode != []:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n",
    "        else:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n",
    "    print(\"Number of NaN : \",df[col_name].isnull().sum())\n",
    "    print(\"------------------\")\n",
    "    print(df[col_name].value_counts(dropna=False))\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def fill_prop(df, group_col, col_name):\n",
    "    for group in list(df[group_col].unique()):\n",
    "        cond = df[group_col]==group\n",
    "        df.loc[cond, col_name] = df.loc[cond, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df[col_name] = df[col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    print(\"Number of NaN : \",df[col_name].isnull().sum())\n",
    "    print(\"------------------\")\n",
    "    print(df[col_name].value_counts(dropna=False))\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def fill(df, group_col1, group_col2, col_name, method): # method can be \"mode\" or \"median\" or \"ffill\"\n",
    "    if method == \"mode\":\n",
    "        for group1 in list(df[group_col1].unique()):\n",
    "            for group2 in list(df[group_col2].unique()):\n",
    "                cond1 = df[group_col1]==group1\n",
    "                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n",
    "                mode1 = list(df[cond1][col_name].mode())\n",
    "                mode2 = list(df[cond2][col_name].mode())\n",
    "                if mode2 != []:\n",
    "                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond2][col_name].mode()[0])\n",
    "                elif mode1 != []:\n",
    "                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond1][col_name].mode()[0])\n",
    "                else:\n",
    "                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[col_name].mode()[0])\n",
    "                \n",
    "    elif method == \"median\":\n",
    "        for group1 in list(df[group_col1].unique()):\n",
    "            for group2 in list(df[group_col2].unique()):\n",
    "                cond1 = df[group_col1]==group1\n",
    "                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n",
    "                df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond2][col_name].median()).fillna(df[cond1][col_name].median()).fillna(df[col_name].median())\n",
    "                \n",
    "    elif method == \"ffill\":           \n",
    "        for group1 in list(df[group_col1].unique()):\n",
    "            for group2 in list(df[group_col2].unique()):\n",
    "                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n",
    "                df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "                \n",
    "        for group1 in list(df[group_col1].unique()):\n",
    "            cond1 = df[group_col1]==group1\n",
    "            df.loc[cond1, col_name] = df.loc[cond1, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")            \n",
    "           \n",
    "        df[col_name] = df[col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    \n",
    "    print(\"Number of NaN : \",df[col_name].isnull().sum())\n",
    "    print(\"------------------\")\n",
    "    print(df[col_name].value_counts(dropna=False))\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3721087",
   "metadata": {},
   "source": [
    "# Load|Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('.csv')\n",
    "df = df0.copy()\n",
    "df.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e5c3a",
   "metadata": {},
   "source": [
    "# First Looking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d06787",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_looking(df)\n",
    "duplicate_values(df)\n",
    "drop_columns(df, [])\n",
    "drop_null(df, 90)\n",
    "# df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6238cc1",
   "metadata": {},
   "source": [
    "# Train-Test Split|Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f94295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dtype_object = df[['categorical1','categorical2']].astype('object')  # if we have features need to be dummy!!!\n",
    "X_numerical = df.drop(target, axis=1).select_dtypes('number').astype('float64')\n",
    "X_categorical = df.drop(target, axis=1).select_dtypes('object')\n",
    "\n",
    "if (df.dtypes==object).any():\n",
    "    dummied = pd.get_dummies(X_categorical, drop_first=True)\n",
    "    X = pd.concat([X_numerical, dummied[dummied.columns]], axis=1)\n",
    "    \n",
    "else:\n",
    "    X = df.drop(target, axis=1).astype('float64')\n",
    "\n",
    "y = df[target]\n",
    "\n",
    "###############################################################################\n",
    "## Train - Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y,\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "###############################################################################\n",
    "## Scaling\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "pd.DataFrame(X_train_scaled, columns=X.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac469b",
   "metadata": {},
   "source": [
    "# OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, drop=\"if_binary\")\n",
    "ohe_df = ohe.fit_transform(df_[X_categorical.columns])\n",
    "ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names(df_[X_categorical.columns].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cc476",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_to = 100000\n",
    "over = SMOTE(sampling_strategy={1:over_to})\n",
    "under = RandomUnderSampler(sampling_strategy={0:(y_train.value_counts().sum()-over_to)})\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "ros1 = SMOTETomek()\n",
    "ros2 = SMOTEENN()\n",
    "X_resampled_, y_resampled_ = ros1.fit_resample(X_train_scaled, y_train)\n",
    "X_resampled__, y_resampled__ = ros2.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Over Sampling:\", y_resampled.value_counts())\n",
    "print(\"Omek:\", y_resampled_.value_counts())\n",
    "print(\"ENN\", y_resampled__.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222cd19",
   "metadata": {},
   "source": [
    "# Model Selection 1 - Train Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X_train, y_train):\n",
    "    # Logistic Regression\n",
    "    log = LogisticRegression(class_weight=\"balanced\", random_state=42)\n",
    "    log.fit(X_train, y_train)\n",
    "    # Decision Tree\n",
    "    decision_tree = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    # Random Forest\n",
    "    random_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    # KNN\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    # SVC\n",
    "    svc = SVC(class_weight=\"balanced\", random_state=42)\n",
    "    svc.fit(X_train, y_train)\n",
    "    # XGB\n",
    "    xgb = XGBClassifier(random_state=42)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    # AdaBoosting\n",
    "    ab = AdaBoostClassifier(random_state=42)\n",
    "    ab.fit(X_train, y_train)\n",
    "    # GB GradientBoosting\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "#     # AdaCost\n",
    "#     adac = AdaCost(algorithm = \"SAMME\", cost_matrix = cost_matrix, random_state=42)\n",
    "#     adac.fit(X_train, y_train)\n",
    "   \n",
    "\n",
    "    # Model Accuracy on Training Data\n",
    "    print(f\"\\033[1m1) Logistic Regression Training Accuracy:\\033[0m {log.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m2) Decision Tree Training Accuracy:\\033[0m {decision_tree.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m3) Random Forest Training Accuracy:\\033[0m {random_forest.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m4) KNN Training Accuracy:\\033[0m {knn.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m5) SVC Training Accuracy:\\033[0m {svc.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m6) XGBoosting Training Accuracy:\\033[0m {xgb.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m7) AdaBoosting Training Accuracy:\\033[0m {ab.score(X_train, y_train)}\")\n",
    "    print(f\"\\033[1m8) GradiendBoosting Training Accuracy:\\033[0m {gb.score(X_train, y_train)}\")\n",
    "    #print(f\"\\033[1m9) AdaCost Training Accuracy:\\033[0m {adac.score(X_train, y_train)}\")\n",
    "    return log, decision_tree, random_forest, knn, svc, xgb, ab, gb #, adac\n",
    "\n",
    "model_selection(X_train_scaled, y_train_sccaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516af558",
   "metadata": {},
   "source": [
    "# Model Selection 2 - Train|Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer([(\"ohe\", OneHotEncoder(drop=\"if_binary\"), X_categorical.columns),\n",
    "                                 (\"scaler\", StandardScaler(), X_numerical.columns)], \n",
    "                                 remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append((\"LOG\", LogisticRegression(class_weight=\"balanced\", random_state=42)))\n",
    "models.append((\"DTC\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)))\n",
    "models.append((\"RFC\", RandomForestClassifier(class_weight=\"balanced\", random_state=42)))\n",
    "models.append((\"KNN\", KNeighborsClassifier()))\n",
    "models.append((\"SVC\", SVC(class_weight=\"balanced\", random_state=42)))\n",
    "models.append((\"ADA\", AdaBoostClassifier(random_state=42)))\n",
    "models.append((\"GBC\", GradientBoostingClassifier(random_state=42)))\n",
    "# evaluate each model in turn\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    pipe = Pipeline([(\"transformer\", transformer),\n",
    "                     (\"model\", model)])\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=\"recall\")\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"{name}: {round(cv_results.mean(), 4)}\")\n",
    "\n",
    "    y_pred = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\n",
    "result_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n",
    "\n",
    "compare = pd.DataFrame({\"F1\": f1_scores,\n",
    "                        \"Recall\": recall_scores,\n",
    "                        \"ROC AUC\": roc_auc_scores\n",
    "                       }, index=names)\n",
    "\n",
    "for score in compare.columns:\n",
    "    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Score\")\n",
    "    \n",
    "compare "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41841e",
   "metadata": {},
   "source": [
    "# Model Selection 3 - Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3712cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycaret[full]\n",
    "!pip install pycaret-nightly\n",
    "# # https://pycaret.readthedocs.io/en/latest/index.html#\n",
    "# # install the nightly build\n",
    "# pip install pycaret-nightly\n",
    "# # install the full version of the nightly build\n",
    "# pip install pycaret-nightly[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "fraud_classifier = setup(df, \n",
    "                         target='class',\n",
    "                         session_id=123,\n",
    "                         train_size=0.8,\n",
    "                         log_experiment=True,\n",
    "                         log_plots=True,\n",
    "                         html=False,\n",
    "                         experiment_name='Creditcard_Fraud_Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec873e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_scores = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f516e22",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3475ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer([(\"ohe\", OneHotEncoder(drop=\"if_binary\"), X_categorical_list),\n",
    "                                 (\"scaler\", StandardScaler(), X_numerical_list)], \n",
    "                                 remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2143d",
   "metadata": {},
   "source": [
    "# Catboost, XGBoost, LightGBM-Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "# https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_recall = []\n",
    "comp_recall_name = []\n",
    "X_categorical_list=X_categorical.columns.to_list()\n",
    "X_numerical_list=X_numerical.columns.to_list()\n",
    "X_columns_list=X.columns.to_list()\n",
    "\n",
    "transformer = ColumnTransformer([(\"ohe\", OneHotEncoder(drop=\"if_binary\"), X_categorical_list),\n",
    "                                 (\"scaler\", StandardScaler(), X_numerical_list)], \n",
    "                                 remainder=\"passthrough\")\n",
    "\n",
    "models = []\n",
    "models.append((\"XGB\", XGBClassifier(random_state=42, verbosity = 0, scale_pos_weight = 600)))\n",
    "models.append((\"LGB\", LGBMClassifier(random_state=42, scale_pos_weight = 600)))\n",
    "models.append((\"CAT\", CatBoostClassifier(random_state=42, verbose=0, \n",
    "                                         cat_features=X_categorical_list, scale_pos_weight = 600)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "roc_auc_scores = []\n",
    "precision_recall_auc_scores = []\n",
    "\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    if name != \"CAT\":\n",
    "        pipe = Pipeline([(\"transformer\", transformer),\n",
    "                         (\"model\", model)])\n",
    "        kfold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "        cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=\"recall\")\n",
    "\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "\n",
    "        print(f\"{name} MODEL: {round(cv_results.mean(), 4)}\")\n",
    "        y_pred = pipe.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred, pos_label=0))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "        # calculate the precision-recall auc\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        precision_recall_auc_scores.append(auc(recall, precision))\n",
    "        \n",
    "        \n",
    "        comp_recall.append(recall_score(y_test, y_pred))\n",
    "        comp_recall_name.append(f\"{name} Scale Post Weight 600\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        kfold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"recall\")\n",
    "\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "\n",
    "        print(f\"{name} MODEL: {round(cv_results.mean(), 4)}\")\n",
    "        \n",
    "        y_pred = model.fit(X_train, y_train,cat_features=X_categorical_list).predict(X_test)\n",
    "\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred, pos_label=0))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "        # calculate the precision-recall auc\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        precision_recall_auc_scores.append(auc(recall, precision))\n",
    "        \n",
    "        \n",
    "        comp_recall.append(recall_score(y_test, y_pred))\n",
    "        comp_recall_name.append(f\"{name} Scale Pos Weight 600\")\n",
    "    \n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\n",
    "result_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n",
    "\n",
    "compare = pd.DataFrame({\"F1\": f1_scores,\n",
    "                        \"Recall-1\": recall_scores,\n",
    "                        \"Precision-0\": precision_scores,\n",
    "                        \"ROC AUC\": roc_auc_scores,\n",
    "                        \"Recall AUC\": precision_recall_auc_scores\n",
    "                       }, index=names)\n",
    "\n",
    "for score in compare.columns:\n",
    "    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Score\")\n",
    "    \n",
    "compare\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
